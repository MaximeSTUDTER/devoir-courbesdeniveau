{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir de Calcul Différentiel : Courbes de niveau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dépendances logicielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Libraries\n",
    "import math\n",
    "from math import isclose\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-Party Libraries\n",
    "\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "from autograd.numpy import linalg\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différentiation automatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f):\n",
    "    g = autograd.grad\n",
    "    def grad_f(x, y):\n",
    "        return np.array([g(f, 0)(x, y), g(f, 1)(x, y)])\n",
    "    return grad_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(f):\n",
    "    j = autograd.jacobian\n",
    "    def J_f(x, y):\n",
    "        return np.array([j(f, 0)(x, y), j(f, 1)(x, y)]).T\n",
    "    return J_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble de niveaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des ensembles de niveaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $c \\in \\mathbb{R}$, l'ensemble $\\{c\\}$ est un fermé. La fonction $f$ étant continue, la courbe de niveau $c$, qui est en fait $f^-1(\\{c\\})$ est également un fermé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le vecteur $T = \\frac {1} {\\|\\nabla f(x_0)\\|} \\begin{pmatrix} \\partial_2 f(x_0) \\\\ -\\partial_1 f(x_0) \\end{pmatrix}$ est le vecteur normalisé orthogonal au gradient donc tangent à la courbe au point $x_0$. Ainsi, $p(x_1,x_2)$ correspond au produit scalaire entre $T$ et le vecteur $MM_0 = \\begin{pmatrix} x_1 - x_{10} \\\\ x_2 - x_{20} \\end{pmatrix}$, ce qui géométriquement correspond à la projection orthogonale de $MM_0$ sur  $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "On pose $ \\phi : \\mathbb{R}^2 \\times \\mathbb{R} \\to \\mathbb{R}^2$ telle que $ \\forall (x,t) \\in \\mathbb{R}^2 \\times \\mathbb{R},  \\phi(x,t) = \\begin{pmatrix} f(x) - c \\\\ t - p(x) \\end{pmatrix}$. $f$ est continûment différentiable par hypothèse, et $\\phi$ l'est car polynomiale en les coefficents de $x$.   \n",
    "     \n",
    "Vérifions les hypothèses du théorème des fonctions implicites :  \n",
    "- $\\phi(x_0,p(x_0)) = 0$ \n",
    "- Soit $\\partial_x \\phi(x_0,p(x_0))= \\begin{pmatrix} \\partial_1 f(x_0)& \\partial_2 f(x_0) \\\\ -\\frac {\\partial_2 f(x_0)} {\\|\\nabla f(x_0)\\|} & \\frac {\\partial_1 f(x_0)} {\\|\\nabla f(x_0)\\|} \\end{pmatrix}$.    \n",
    "       \n",
    "On remarque alors que $det(\\partial_x \\phi(x_0,p(x_0))) = \\|\\nabla f(x_0)\\| \\ne 0$, donc $\\partial_x \\phi(x_0,p(x_0))$ est bien inversible.    \n",
    "    \n",
    "Le théorème dans sa version étendue s'applique, donc il existe un voisinage ouvert de $p(x_{10},x_{20})=0$, c'est-à-dire un intervalle $]-\\varepsilon,\\varepsilon[$, avec $\\varepsilon > 0$ et un voisinage ouvert $U$ de $x_0$ et $\\gamma : \\mathbb{R} \\to \\mathbb{R}^2$ continûment différentiable telle que $\\forall t \\in ]-\\varepsilon,\\varepsilon[ $ et $ \\forall x \\in U \\ \\phi(x,t) = 0 \\Leftrightarrow (x_1,x_2) = \\gamma(t)$, et comme lorsque $\\phi(x,t)=0, \\ t=p(x)$, on obtient bien le résultat demandé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "Soit $t \\in ]-\\varepsilon,\\varepsilon[$,\n",
    "\n",
    "- Montrons que $\\gamma'(t)$ est non nul :      \n",
    "Le théorème des fonctions implicites, que l'on a utilisé à la question précédente pour obtenir la fonction $\\gamma$, nous donne :      \n",
    "$ \\forall t \\in ]-\\varepsilon,\\varepsilon[, \\ \\gamma'(t) = [\\partial_x \\phi(y,t)]^{-1} \\cdot \\partial_t \\phi(y,t) $, avec $y = p(x_1,x_2)$.        \n",
    "Or, $\\forall (x,t) \\in \\mathbb{R}^2 \\times \\mathbb{R} , \\ \\partial_t \\phi(x,t) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, ce qui signifie que $\\gamma'(t)$ est la deuxième colonne de $[\\partial_x \\phi(y,t)]^{-1}$, qui est effectivement non nul car sinon le déterminant de $[\\partial_x \\phi(y,t)]^{-1}$ serait nul.\n",
    "\n",
    "             \n",
    "- Montrons que $\\gamma'(t)$ est orthogonal à $\\nabla f(\\gamma(t))$, c'est-à-dire que $<\\nabla f(\\gamma(t)),\\gamma(t)> \\, = 0$ :      \n",
    "Par définition, $\\forall t \\in ]-\\varepsilon,\\varepsilon[, \\ f \\circ \\gamma(t) \\, = c$, donc en différenciant, on obtient : \n",
    "$ \\nabla f(\\gamma(t))^{T} \\cdot \\gamma'(t) \\, = 0 $, ce qui est bien le résultat voulu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction des courbes de niveau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "eps = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Newton est peu sensible aux erreurs d'arrondi car il converge rapidement pour autant qu'on soit relativement proche de la racine cherchée. Par conséquent, il n'y a pas besoin de choisir une valeur de `eps` trop petite. Comme dans l'exemple du cours sur les différences finies, on peut prendre `eps = 1e-8` pour éviter à la fois les erreurs de troncature et d'arrondi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tâche 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(F, x0, y0, eps = eps, N = N) :\n",
    "    \n",
    "    for i in range(N) :\n",
    "        \n",
    "        J_F = J(F)(x0, y0)\n",
    "        X0 = np.array([[x0], [y0]])\n",
    "        M = linalg.inv(J_F)\n",
    "        \n",
    "        X = X0 - M.dot(F(x0, y0))\n",
    "        x, y = X[0], X[1]        \n",
    "        \n",
    "        if np.sqrt((x - x0)**2 + (y - y0)**2) <= eps or abs(F(x,y)) <= eps :\n",
    "            return x, y\n",
    "        \n",
    "        x0, y0 = x, y\n",
    "    \n",
    "    raise ValueError(f\"no convergence in {N} steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tâche 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": [
     "level_advanced"
    ]
   },
   "outputs": [],
   "source": [
    "Points_ref = np.array([ (0.40, 0.40), (-0.40,-0.40), (-0.50, 0), (0.50,0), (0, -0.50), (0, 0.50), (0.33, -0.33), (-0.33, 0.33)])\n",
    "\n",
    "def f(x, y) :\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return 3.0*x*x - 2.0*x*y + 3.0*y*y - 0.8\n",
    "\n",
    "# On recode Newton en ajoutant la possibilité de mettre une contrainte entre x1 et x2\n",
    "\n",
    "def Newton_2(F, x0, y0, eps = eps, N = N, k = 1) :\n",
    "        \n",
    "    def F_2(F, k) :\n",
    "            \n",
    "        def F2(x, y):\n",
    "            return np.array([F(x,y), x - k*y])\n",
    "            \n",
    "        return F2\n",
    "        \n",
    "    F_2 = F_2(F, k)\n",
    "        \n",
    "    for i in range(N) :\n",
    "        \n",
    "        J_F2 = J(F_2)(x0, y0)\n",
    "        X0 = np.array([[x0], [y0]])\n",
    "        M = linalg.inv(J_F2)\n",
    "\n",
    "        X = X0 - M.dot(F_2(x0, y0))\n",
    "        x, y = X[0][0], X[0][1]      \n",
    "\n",
    "        if np.sqrt((x - x0)**2 + (y - y0)**2) <= eps or np.linalg.norm(F_2(x,y)) <= eps:\n",
    "            return x, y\n",
    "\n",
    "        x0, y0 = x, y\n",
    "\n",
    "    raise ValueError(f\"no convergence in {N} steps.\")\n",
    "\n",
    "def graph_solutions(q = 1):\n",
    "\n",
    "    solution_x = []\n",
    "    solution_y = []\n",
    "    solution = []\n",
    "    for point in Points_ref:\n",
    "        if (2 - 6*q)*point[0] != (6 - 2*q)*point[1]:\n",
    "            try:\n",
    "                x, y = Newton_2(f, point[0], point[1], k = q)\n",
    "                print(x,y)\n",
    "                if isclose(x, q*y):\n",
    "                    if (x, y) not in solution:\n",
    "                        solution.append((x, y))\n",
    "                        solution_x.append(x)\n",
    "                        solution_y.append(y)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "    plt.scatter(solution_x, solution_y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "no convergence in 1000 steps.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-afa9ba51e022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNewton_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-198-0caf0aaefc45>\u001b[0m in \u001b[0;36mNewton_2\u001b[1;34m(F, x0, y0, eps, N, k)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"no convergence in {N} steps.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgraph_solutions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: no convergence in 1000 steps."
     ]
    }
   ],
   "source": [
    "Newton_2(f, 0.1, 0.2, k = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "level_advanced"
    ]
   },
   "source": [
    "### Génération des points\n",
    "\n",
    "#### Question 6 et Tâche 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette question 6, on va chercher les 0 consécutifs de fonctions définies à l'aide des points successifs $(x_0, y_0),\\:  \\cdots,\\: (x_{N-1}, y_{N-1}) $ comme $F_{x_i, y_i} : (x,y) \\in \\mathbb{R}^2 \\mapsto \\begin{pmatrix}(x-x_i)^2 \\: +\\: (y-y_i)^2\\:-\\:  \\delta ^2\\\\ f(x,y)\\: -\\: c\\end{pmatrix}$ \n",
    "\n",
    "  Pour que l'on s'oriente \"à droite\" quand on est en $(x_i, y_i)$ en regardant dans la direction du gradient de f en ce point, on doit aussi ajouter la condition $\\left \\langle \\begin{pmatrix}\\frac{\\partial }{\\partial y}f(x_i, y_i)\\\\ -\\frac{\\partial }{\\partial x}f(x_i, y_i)\\end{pmatrix}, \\begin{pmatrix}x_{i+1}\\: -\\:x_i \\\\ y_{i+1}\\: -\\: y_i\\end{pmatrix} \\right \\rangle\\: > 0,\\:$ avec $i\\: \\in \\: \\{0,\\:  \\cdots,\\:  N-1\\}$ et où $(x_{i+1}, y_{i+1})$ a été déterminé avec la méthode de Newton adaptée à la fonction $F_{x_i, y_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bouclage et auto-intersection\n",
    "\n",
    "#### Question 7 et Tâche 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
